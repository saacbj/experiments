import nanogcg
import torch
import json
import copy
import typer
import gc
import time
import sys
from pathlib import Path
from tabulate import tabulate
from datetime import datetime
from nanogcg import GCGConfig

import logging as log  # logging

"""
Agosto 2025

C√≥digo pensado para correr pruebas del esquema A en el cl√∫ster de CIMAT.

La generaci√≥n de texto se traslada a otro script dentro de la misma carpeta para evitar sobrecargar la VRAM. Lo mismo para la evaluaci√≥n.

Se crea una carpeta output por cada ejecuci√≥n (cada configuraci√≥n) y dos archivos:

El output del c√≥digo es :
- id de conversaci√≥n (para despu√©s conectar con el mensaje)
- target
- best_string
- best_loss
- strings
- losses
"""

# --- Cargamos rutas a directorios de importancia ---
EXPERIMENTS_DIR = Path(__file__).resolve().parent.parent.parent
SCHEME_DIR = Path(__file__).resolve().parent.parent

# --- Importamos el m√≥dulo utils.py ---
sys.append.path(EXPERIMENTS_DIR.as_posix())
from utils import *  # noqa: E402

# --- Creamos directorio OUT ---
# ../results/out_250810_144416
tstamp = datetime.now().strftime("%y%m%d_%H%M%S")
DIRECTORY_OUT = SCHEME_DIR / f"results/out_{tstamp}"
DIRECTORY_OUT.mkdir(parents=True, exist_ok=False)

# ----- Funci√≥n main -----


def main(
  dataset: str = "msc",  # Conjunto de datos a usar (msc √≥ bst)
  num_steps: int = 250,  # N√∫mero de iteraciones
  topk: int = 64,  # N√∫mero de sustituciones de candidatos a considerar en una cierta posici√≥n
  search_width: int = 64,  # N√∫mero de candidatos en cada iteraci√≥n de GCG
  seed: int = 42,  # Para reproductibilidad
  num_conv: int = 5,  # Cu√°ntas conversaciones se van a usar por dataset
  random: bool = False,  # ¬øSe toman aleatoriamente?
  mod: str = "llama-2-7b-chat"  # Qu√© modelo se usa en nanogcg
):

  # Nombre del archivo para el log y el csv
  # e.g. msc_250_64_32_5
  # hasta este momento, es el nombre sin sufijos
  tmstp = f"{dataset}_{num_steps}_{topk}_{search_width}_{num_conv}".lower()
  FILENAME_OUT = DIRECTORY_OUT / tmstp
  if FILENAME_OUT.exists():
    tmstp += "_{uuid.uuid4().hex[:4]}"
    FILENAME_OUT = DIRECTORY_OUT / tmstp
  del tmstp

  # ***** CONFIGURACI√ìN DEL LOGGING *****

  # Formato del logging
  log.basicConfig(
    filename=FILENAME_OUT.with_suffix(".log"),
    filemode="w",  # "a" para no sobrescribir
    format="%(asctime)s - %(message)s",
    level=log.INFO,
    datefmt="%Y %m %d %H:%M:%S"
  )

  # Mensaje inicial
  log.info(f"{FILENAME_OUT.stem}")

  # ***** C√ìDIGO PRINCIPAL *****

  # ----- Configuramos nanogcg -----

  # Configuraci√≥n del GCG
  config = GCGConfig(
    num_steps=num_steps,
    search_width=search_width,
    topk=topk,
    seed=seed,
    verbosity="ERROR"
  )

  # Mostramos los par√°metros en el log
  parameters = {
    "dataset": dataset,
    "num_steps": num_steps,
    "topk": topk,
    "search_width": search_width,
    "seed": seed,
    "num_conv": num_conv,
    "random": random,
    "model": mod
  }
  table = tabulate(
    [parameters],
    tablefmt="simple",
    headers="keys",
    showindex=False,
    numalign="center",
    stralign="center"
  )
  log.info(f"\n{table}")

  # ----- Cargamos modelos y tokenizadores -----

  model, tokenizer = load_model_tokenizer(mod)

  # ----- Datos -----

  # Leemos el contenido del dataset
  conversations = get_dataset(dataset, random, num_conv)

  for conversation in conversations:
    _start_time = time.time()  # Tiempo de ejecuci√≥n

    # Copiamos el mensaje original
    base_message = copy.deepcopy(conversation[5:-1])

    # Quitamos los √∫ltimos 5 por el token de EOS
    target = tokenizer.apply_chat_template(
      conversation[1:5],
      tokenize=False
    )[:-5]

    # A√±adimos el posicionador del adv_string
    # ‚ùì ¬øCon o sin espacio?
    message = copy.deepcopy(base_message)
    message[0]["content"] = '{optim_str} ' + message[0]["content"]

    # ----- Ejecutamos el algoritmo -----

    result = nanogcg.run(
      model,
      tokenizer,
      message,
      target,
      config
    )

    # ----- Juntamos la informaci√≥n -----
    curr_result = {"id": conversation[0]}  # id de conversaci√≥n
    curr_result["target"] = target
    curr_result["best_string"] = result.best_string
    curr_result["best_loss"] = result.best_loss
    curr_result["strings"] = result.strings
    curr_result["losses"] = result.losses

    # --- Guardamos los resultados ---

    with open(FILENAME_OUT.with_suffix(".jsonl"), "a") as f_json:
      json.dump(curr_result, f_json, ensure_ascii=False)
      f_json.write("\n")

    log.info(
      f"üü¢ Conversaci√≥n {conversation[0]} completada. [{(time.time() - _start_time) / 60}min]"
    )

  # ----- Liberamos espacio -----

  del tokenizer, model
  gc.collect()
  torch.cuda.empty_cache()

  log.info("üí¨ Espacio liberado.")


if __name__ == "__main__":
  typer.run(main)
